# Laboratory work nr. 5: Parser & Building an Abstract Syntax Tree

### Course: Formal Languages & Finite Automata
### Author: Frunze Vladislav
### Group: FAF-212

----

## Theory:
* Parsing - taking the tokens generated by the Lexer one by one in the given order, and analyzing if they satisfy the rules of the grammar of the language.
* Abstract syntax tree (AST) - a tree representation of a string of a language, in which the nodes are the leafs-terminals generated by the parser tree. In addition, it doesn't have nodes with tokens of the type separator, and others as well, depending on your requirements.
* Lexical analysis - the process of categorizing the elements of your string; it would output the tokens like (toke_type, token_value (lexeme));
* Regular expression - an expression that is used as a searching pattern in a string; in other words, it is a generalization of the elements that you want to find in the string.


## Objectives:
1. To understand what lexical analysis is.
2. To get familiar with the inner workings of a lexer/scanner/tokenizer.
3. To implement a sample lexer of Python programming language and to show how it works.
4. To read references 1 and 2.
5. To create the report.


## Implementation description:
In the laboratory work nr. 3, the language that I chose was Python.

#### Lexer method with TokenType and regular expressions
In addition, to the lexer method from laboratory nr. 3, I have created another lexer method that uses regular expressions to categorize the elements of the program. First, we create a list that contains the token types and the regular expressions used to identify the type of the element in the program. Next, we create a pattern by joining the regular expressions for all token types and compile the pattern into a regular expression object. With this object, we iterate over matches in the input string and add the tokens to the list.

```  
        token_types = [
            ('NUMBER', r'\d+(\.\d*)?'),            
            ('STRING', r'\".*?\"|\'.*?\''),                  
            ('KEYWORD', r'(if|else|while|for|def|return|in)(?!\w)'), 
            ('IDENTIFIER', r'[a-zA-Z_]\w*'),                 
            ('OPERATOR', r'[+\-*%/]'),                      
            ('COMPARISON', r'==|!=|<=|>=|<|>'),             
            ('ASSIGNMENT', r'='),                            
            ('SEPARATOR', r'\(|\)|\[|\]|\{|\}|,|;|:'),
            ('NEWLINE', r'\n'),                              
            ('SKIP', r'[ \t]+'),                              
            ('COMMENT', r'#.*'),                              
            ('UNKNOWN', r'.'),                                
        ]

        token_pattern = '|'.join('(?P<%s>%s)' % spec for spec in token_types)
        regex = re.compile(token_pattern)

        for match in regex.finditer(self.program):
            token_type = match.lastgroup
            token_value = match.group(token_type)
            if token_type == 'UNKNOWN':
                tokens.append((token_type, ord(token_value)))
            elif token_type != 'SKIP' and token_type != 'NEWLINE':
                tokens.append((token_type, token_value)) 
            
        return tokens
```


#### AST:
To implement the data structure of the AST, we can create a class "ASTNode", that will take the token type and value and of course its children nodes.

```
class ASTNode:
    def __init__(self, token_type, value=None):
        self.token_type = token_type
        self.value = value
        self.children = []

    def add_child(self, child):
        self.children.append(child)
        return child

    def __str__(self):
        return f'{self.token_type}: {self.value}'
```

For the below Python function from lab. 3, I can create the AST in 2 different ways, first is to manually create an AST, and, second, to use the "ast" module in Python.

```
def sum_list(numbers):
    total = 0
    for num in numbers:
        total = total + num
    return total
```

First method suggests to create the AST for the above program manually in the following way. The AST will be represented in the "Results" section.
```
        root = ASTNode('PROGRAM')
        current_node = root
        current_node = current_node.add_child(ASTNode(tokens[0][0], tokens[0][1]))
        current_node.add_child(ASTNode(tokens[1][0], tokens[1][1]))
        current_node.add_child(ASTNode(tokens[3][0], tokens[3][1]))
        current_node = current_node.add_child(ASTNode('body', None))
        parent = current_node
        current_node = current_node.add_child(ASTNode(tokens[7][0], tokens[7][1]))
        current_node.add_child(ASTNode(tokens[6][0], tokens[6][1]))
        current_node.add_child(ASTNode(tokens[8][0], tokens[8][1]))
        current_node = parent
        current_node = current_node.add_child(ASTNode(tokens[9][0], tokens[9][1]))
        current_node.add_child(ASTNode(tokens[10][0], tokens[10][1]))
        current_node.add_child(ASTNode(tokens[11][0], tokens[11][1]))
        current_node.add_child(ASTNode(tokens[12][0], tokens[12][1]))
        current_node = current_node.add_child(ASTNode(tokens[15][0], tokens[15][1]))
        current_node.add_child(ASTNode(tokens[14][0], tokens[14][1]))
        current_node = current_node.add_child(ASTNode(tokens[17][0], tokens[17][1]))
        current_node.add_child(ASTNode(tokens[16][0], tokens[16][1]))
        current_node.add_child(ASTNode(tokens[18][0], tokens[18][1]))
        current_node = parent
        current_node = current_node.add_child(ASTNode(tokens[19][0], tokens[19][1]))
        current_node.add_child(ASTNode(tokens[20][0], tokens[20][1]))

        return root
```


The second method to create the AST for the above Python program is to use the "ast" module. First, parse the source code into an AST using the 'ast' module. Then, print the AST for debugging or analysis. The AST will be represented in the "Results" section.

```
    def create_ast_method_2(source_code):
        tree = ast.parse(source_code)
        print(ast.dump(tree))
```


#### Parser (class) program:
As it would have taken way too much time to create the entire parser for the Python programming language, I had to compromise and create a parser for simple Python statements.

The parser class, would take the list of tokens to parse, current token being parsed, and the current index of the token being parsed.

```
        self.tokens = tokens 
        self.current_token = None
        self.index = -1
        self.next_token()
```

Next, we create a method to move to the next token in the list given by the Lexer.

```
        self.index += 1
        if self.index < len(self.tokens):
            self.current_token = self.tokens[self.index]
        else:
            self.current_token = None
```

Then, we create a method to check if the current token matches the expected token. If it does, we move to the next token, otherwise we raise a syntax error.

```
        if self.current_token and self.current_token[0] == expected_token:
            self.next_token()
        else:
            raise SyntaxError(f"Expected {expected_token}, but found {self.current_token[0]}")
```

The next method that we have to create is the one that parses an expression consisting of terms and operators.

```
        self.parse_term()
        while self.current_token and self.current_token[0] == 'OPERATOR':
            self.match('OPERATOR')
            self.parse_term()
```

Then, we create the method called by the above one. It will parse a term, which can be an identifier or a number. If it is not, it will raise a syntax error.

```
        if self.current_token and self.current_token[0] in ['IDENTIFIER', 'NUMBER']:
            self.match(self.current_token[0])
        else:
            raise SyntaxError(f"Expected IDENTIFIER or NUMBER, but found {self.current_token[0]}")
```


We also write a method to parse the assignment statement. Thus, it will expect an identifier, an assignment operator, and the expression on the right-hand side of the assignment.

```
        self.match('IDENTIFIER')
        self.match('ASSIGNMENT')
        self.parse_expression()
```

Then, we write the general method to parse the statement.

```
        self.parse_assignment()
        if self.current_token:
            raise SyntaxError(f"Unexpected token: {self.current_token[0]}")
```

Finally, we write the last method, which will output weather the program is written correctly or not.

```
        try:
            self.parse_statement()
            print("Parsing successful. The statement is correctly written.")
        except SyntaxError as e:
            print("Parsing failed. The statement is incorrectly written.")
            print(e)
```



## Conclusions / Screenshots / Results
### Results:
<b>Result 1: get_tokens_with_regex()</b>
```
    Input:
    def sum_list(numbers):
        total = 0
        for num in numbers:
            total = total + num
        return total

    Output:
    ('KEYWORD', 'def')
    ('IDENTIFIER', 'sum_list')
    ('SEPARATOR', '(')
    ('IDENTIFIER', 'numbers')
    ('SEPARATOR', ')')
    ('SEPARATOR', ':')
    ('IDENTIFIER', 'total')
    ('ASSIGNMENT', '=')
    ('NUMBER', '0')
    ('KEYWORD', 'for')
    ('IDENTIFIER', 'num')
    ('KEYWORD', 'in')
    ('IDENTIFIER', 'numbers')
    ('SEPARATOR', ':')
    ('IDENTIFIER', 'total')
    ('ASSIGNMENT', '=')
    ('IDENTIFIER', 'total')
    ('OPERATOR', '+')
    ('IDENTIFIER', 'num')
    ('KEYWORD', 'return')
    ('IDENTIFIER', 'total')
```

<b>Result 2: AST created with the first method</b><br>
Output:
![The AST generated by the above:](https://github.com/Frunnze/LFAF_Labs/blob/main/reports/lab_5_report/ast_graph.png)


<b>Result 3: AST created with the second method</b><br>
Output:
```
Module(
    body=[
        FunctionDef(name='sum_list', args=arguments(posonlyargs=[], args=[arg(arg='numbers')], kwonlyargs=[], kw_defaults=[], defaults=[]), 
            body=[
                    Assign(targets=[Name(id='total', ctx=Store())], value=Constant(value=0)), 
                    For(target=Name(id='num', ctx=Store()), iter=Name(id='numbers', ctx=Load()), 
                        body=[Assign(targets=[Name(id='total', ctx=Store())], value=BinOp(left=Name(id='total', ctx=Load()), op=Add(), right=Name(id='num', ctx=Load())))], orelse=[]), 
                    Return(value=Name(id='total', ctx=Load()))
                ], decorator_list=[])
        ], type_ignores=[]
    )
```

<b>Result 4: Parsing</b>
```
    Input:
    total = total + num

    Output:
    Parsing successful. The statement is correctly written.
```

<b>Result 5: Parsing</b>
```
    Input:
    total = + + num

    Output:
    Parsing failed. The statement is incorrectly written.
    Expected IDENTIFIER or NUMBER, but found OPERATOR
```

<b>Result 6: Parsing</b>
```
    Input:
    total = 34 + num * 43

    Output:
    Parsing successful. The statement is correctly written.
```

### Conclusions:
* AST is a helpful way of analyzing the direct structure and elements of your language.
* Regular expressions reduce the length of your Lexer method significantly.
* Parsing is a crucial part of a programming language.
* The main difference between an AST and a parse tree is that the parse tree has non-terminals in the nodes.

## References
1. "Parsing" Wikipedia: https://en.wikipedia.org/wiki/Parsing
2. "Abstract syntax tree" Wikipedia: https://en.wikipedia.org/wiki/Abstract_syntax_tree